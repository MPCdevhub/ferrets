{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Angry Ferret Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Imports and Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import subprocess\n",
    "\n",
    "import PIL\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.pytorch import PyTorch, PyTorchModel\n",
    "from sagemaker.predictor import RealTimePredictor, json_deserializer\n",
    "\n",
    "from fastai.vision import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = 'angryferrets'\n",
    "prefix = 'OwnModel/data'\n",
    "\n",
    "inputs = 's3://'+ bucket + '/' + prefix\n",
    "print('input spec (in this case, just an S3 path): {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Training Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are preparing a model for publication into the AWS Marketplace, a few things you will need to keep in mind:\n",
    "- At present, you can only submit models to AWS Marketplace from the ```us-east-2``` region.\n",
    "- When you submit your model, you must go through an automated validation process.  This requires your model to be able to perform batch inferences so be sure your inference code can handle this.\n",
    "- When the validation process happens, your model will be placed in a container without internet access, so it will have no ablility to download models, URLs or code from the internet.  This is important as some default implementations of certain frameworks will try to download pretrained models from zoos for example.  You need to stage the files within the container itself or disable the models from trying to download pretrained elements.\n",
    "\n",
    "To this last point, you'll notice in the training code below, I am using a pretrained model for the training process as we're training this just using normal SageMaker.  Then, we'll be taking our trained model and creating a model package from that training job.  The model we're publishing to the AWS Marketplace is designed to only infer so we don't need to download anything...which allows us to validate in a container without internet access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize container/src/ferrets.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our Docker Container for Training and Inference\n",
    "For this example, we are creating a customized container based on one provided by AWS for PyTorch.  We pull a specific version from the public repo and then add in the additional parts we need.  In this case, the version of FastAI is a little old so we want to update with a more recent version.  (Notice ```RUN pip install --no-cache-dir fastai==1.0.54 --upgrade```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat container/Dockerfile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat container/build_docker.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a bash script to build the docker images and push it to our Elastic Container Repository.  I recommend you execute this script in a Terminal session so you can see what's going on.\n",
    "\n",
    "```\n",
    "cd container\n",
    "chmod +x build_docker.sh\n",
    "./build_docker.sh <<containername>>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Test Locally\n",
    "Now we have our customized Docker image both locally and in our remote repo.  Let's test it a bit by training and infering locally.  This will save you lots of time on iterations as you troubleshoot versus trying to launch instances on SageMaker.\n",
    "\n",
    "I highly recommend springing for a P2 or P3 instances for training.  Otherwise, it will take quite a while.\n",
    "\n",
    "As such, the Docker images we've built are for GPU instances, but they will run on CPU instances.  In fact, we're going to use a CPU instance for inference for our eventual deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train locally\n",
    "instance_type = 'local_gpu'\n",
    "data_location = 'file://./data/'\n",
    "\n",
    "print(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to update our Docker process to organize it for local training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo cp ./container/daemon.json /etc/docker/daemon.json && sudo pkill -SIGHUP dockerd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start the training process using the ```Estimator``` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator(role=role,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type=instance_type,\n",
    "                      image_name='angryferrets:latest')\n",
    "\n",
    "estimator.fit(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a while, you'll see that our training process has finished.  We can now create a local endpoint to test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(1, instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets send some test images into the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './images/test/angry_test1.jpg'\n",
    "#filename = './images/test/nice_test2.jpg'\n",
    "#filename = './images/test/angry_test2.jpg'\n",
    "#filename = './images/test/nice_test1.jpg'\n",
    "\n",
    "img = PIL.Image.open(filename,mode='r')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.content_type = 'image/jpeg'\n",
    "response = predictor.predict(open(filename, 'rb'))\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we're satisfied, we can delete our local endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Hosting on SageMaker\n",
    "This process is much the same as training and deploying locally, but notice that we're using our S3 bucket for ```data_location``` and our ```image_name``` is the path to our ECR image that we pushed.\n",
    "This process will take quite a bit longer than the local training did because SageMaker has to provision new instances and containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location=inputs\n",
    "instance_type = 'ml.p3.2xlarge'\n",
    "\n",
    "# ECR path from our bash script above that pushed our image to our repo\n",
    "ecr_path = '<<insert the ECR path to your container>>'\n",
    "\n",
    "print (data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator(role=role,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type=instance_type,\n",
    "                      image_name=ecr_path)\n",
    "\n",
    "estimator.fit(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can deploy it on SageMaker.  Notice that I'm using a CPU instance here, which is perfectly fine.  Many times, the training process benefits greatly from a GPU instance due to the highly computational nature of training.  For inference, you can usually get by with a smaller class instance and thus save some money."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = 'ml.c5.large'\n",
    "\n",
    "predictor = estimator.deploy(initial_instance_count=1,\n",
    "                                       instance_type=instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we see the ```--!``` we know the endpoint is deployed.  If you get an ```---*```, something has gone wrong.  You'll need to check the logs in CloudWatch to see what happened.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = './images/test/angry_test1.jpg'\n",
    "#ilename = './images/test/nice_test2.jpg'\n",
    "#filename = './images/test/angry_test2.jpg'\n",
    "filename = './images/test/nice_test1.jpg'\n",
    "\n",
    "img = PIL.Image.open(filename,mode='r')\n",
    "img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.content_type = 'image/jpeg'\n",
    "response = predictor.predict(open(filename, 'rb'))\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
